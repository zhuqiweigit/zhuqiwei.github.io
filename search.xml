<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hexo notes</title>
    <url>/2021/02/23/hexo%20notes/</url>
    <content><![CDATA[<h1 id="hexo-notes"><a href="#hexo-notes" class="headerlink" title="hexo notes"></a>hexo notes</h1><span id="more"></span>

<ul>
<li><p>新建文章</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;hexo notes&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>编译生成页面</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
</li>
<li><p>本地开启服务器并预览</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo server   <span class="comment">#一般会在http://localhost:4000</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>deploy（一键git提交、push到远端）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>
</li>
<li><p>Clean 清理缓存文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
</li>
<li><p>markdown中存在图片：</p>
<p>将pic拷贝到 source/images目录下，并将md中的引用链接也设置为此</p>
</li>
<li><p>主页文章缩略</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">在文章内添加 &lt;!-- more --&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>动态加载图片</p>
<p>在config中开启pjax</p>
</li>
</ul>
]]></content>
      <categories>
        <category>其它</category>
      </categories>
  </entry>
  <entry>
    <title>lab1-mapReduce</title>
    <url>/2021/03/29/lab1-mapReduce/</url>
    <content><![CDATA[<p>最近开始学习MIT的分布式课程，首先做了lab1。</p>
<p>lab1的内容是mapReduce，关键有两部分，一是理解mapReduce的原理，二是实现分布式的mapReduce。</p>
<p>以下内容也从这两部分着手</p>
<span id="more"></span>

<h2 id="MapReduce-的原理"><a href="#MapReduce-的原理" class="headerlink" title="MapReduce 的原理"></a>MapReduce 的原理</h2><p>下图是一个word count 程序在mapReduce上的原理图。本图仅仅涉及worker节点，master节点未画出。master节点主要起 给worker节点分配工作的作用。</p>
<p><img src="../images/mapreduce.jpg" alt="mapreduce"></p>
<p>顾名思义，mapReduce就是将<strong>可分割</strong>的任务划分成两个阶段完成</p>
<ul>
<li>map阶段将原始数据(如文件内容)拆分成了一个个key-value pair的形式</li>
<li>reduce阶段将这些拥有相同key的pair聚集在一起，进行某种用户自定义的汇总运算。</li>
</ul>
<p>mapReduce 的例子</p>
<ul>
<li>单词计数：map函数将content拆分成 {word, “1”}的形式；reduce将聚集在一起的相同的word的进行频率计数，输出{word, “1023”}的形式</li>
<li>倒排索引：由单词找文档。map函数将{docName, content} 拆分为 {word, docName}的list；reduce将聚集在一起的相同的word，的对应的docName进行排序并汇总</li>
</ul>
<h2 id="分布式的MapReduce"><a href="#分布式的MapReduce" class="headerlink" title="分布式的MapReduce"></a>分布式的MapReduce</h2><h3 id="1-master"><a href="#1-master" class="headerlink" title="1. master"></a>1. master</h3><p>分布式的MapReduce采用C/S的模式，其中master作为服务端，worker节点作为客户端。即分布式MapReduce的运行模式为：worker节点通过RPC不断向master节点请求任务，master节点根据自己的数据结构记录的任务情况来给worker节点分配任务(通过RPC返回任务的meta description)。worker节点获取任务meta信息后，通过分布式文件系统获取文件，并完成任务的计算。</p>
<p>Master的主要数据结构如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Master <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//待处理的n个文件</span></span><br><span class="line">	files []<span class="keyword">string</span></span><br><span class="line">	</span><br><span class="line">  <span class="comment">//map任务的meta描述，凭借这个来分配map任务</span></span><br><span class="line">	mapTasks   []Task</span><br><span class="line">	mapTaskPtr <span class="keyword">int</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">//reduce任务的meta描述，凭借这个来分配reduce任务</span></span><br><span class="line">	reduceTasks   []Task</span><br><span class="line">	reduceTaskPtr <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//当前已完成的task计数，用于判断每个阶段的任务是否完成</span></span><br><span class="line">	mapCompleteCnt <span class="keyword">int</span></span><br><span class="line">	reduceCompleteCnt <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//mutex互斥保护变量：RPC是并发的，因此需要互斥访问Master的meta data</span></span><br><span class="line">	mutex sync.Mutex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>描述任务的数据结构如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Task <span class="keyword">struct</span> &#123;</span><br><span class="line">  <span class="comment">//任务类型，分为：Map任务，Reduce任务，IDLE空闲任务</span></span><br><span class="line">	taskType_ <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line">	taskId_ <span class="keyword">int</span></span><br><span class="line">	<span class="comment">//任务状态：待分配、已分配、已完成</span></span><br><span class="line">	taskState_ <span class="keyword">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Master主要需要实现两个RPC接口供Worker调用：</p>
<ul>
<li>AskForTask：请求任务，该接口会根据任务分配和完成情况，返回一个task。具体来说，当所有map任务<strong>完成</strong>后，才可以继续分配reduce任务。如果所有map任务都已经分配出去，但并没有全部收到commit，说明部分map任务还在被执行，此时如果有worker请求任务，应该给它分配IDLE任务。</li>
<li>CommitTask：提交任务，该接口会将一个已分配任务的状态改为已完成</li>
</ul>
<h3 id="2-worker"><a href="#2-worker" class="headerlink" title="2. worker"></a>2. worker</h3><p>worker作为客户端，不断调用Master提供的AskForTask请求任务，当获得一个任务后，就开始执行，执行完毕后调用CommitTask提交任务。然后循环继续请求新的任务。</p>
<p>worker每次请求任务都是通过RPC，当RPC向Master节点发起连接时，如果连接失败，说明master损坏，or整个mapReduce任务已经完成且master已退出。此时worker也退出即可。</p>
<p>worker执行任务的流程则如文章一开始画的图一样。</p>
<h3 id="3-错误处理"><a href="#3-错误处理" class="headerlink" title="3. 错误处理"></a>3. 错误处理</h3><p>master故障：只需要将master节点的meta-data信息周期性写入磁盘，当master故障时，重启一个新节点并load这个meta-data即可继续执行master的功能。当然一般来说master是不允许有故障的，如果出现了故障，直接停止整个程序的运行，让用户检查故障也是一种方法</p>
<p>worker故障：如果worker发生了故障，则worker的任务需要由其它worker重新完成</p>
<ul>
<li>master端：master分配一个任务给worker后，会启动一个协程来监控任务的完成情况。具体来说，可以设置一个定时器，当10s后检查该任务是否已经由worker提交，且任务的状态改为“已完成“。如果否，则说明worker可能存在故障，只需要将该任务从 “已分配”的状态改为“待分配”，即可。</li>
<li>worker端：worker的任务提交必须是原子性的，即worker在运行中生成的文件是私有的，可以将其命名为temp文件。只有当任务完成后，才将temp文件改成标准命名的文件，使得其它worker节点可见，然后再向master节点commit任务。这样做的原因是，如果worker在执行任务的过程中直接使用标准命名的文件，当work节点挂掉后，其它work节点会重做该task，但此时出现了file name的冲突。</li>
</ul>
<h3 id="4-go编程的一些note"><a href="#4-go编程的一些note" class="headerlink" title="4. go编程的一些note"></a>4. go编程的一些note</h3><p>刚开始尝试go编程，记录一些go的语法</p>
<ul>
<li><p>RPC</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.RPC传递的struct的成员必须大写</span></span><br><span class="line"><span class="keyword">type</span> TaskRequest <span class="keyword">struct</span> &#123;</span><br><span class="line">	Pad <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.RPC的接口函数格式; 请求消息和返回消息通过函数参数传递，因此需要使用引用传递；返回值必须是error类型，若没有错误则返回nil</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Master)</span> <span class="title">CommitTask</span><span class="params">(request *CommitRequest, response *CommitResponse)</span> <span class="title">error</span></span>&#123;</span><br><span class="line">  	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>fmt 相关</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1. %v可打印各种变量</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;open file [%v] failed  %v&quot;</span>, fileName, err)</span><br><span class="line"></span><br><span class="line"><span class="comment">//2. Sprintf 格式化输出字符串</span></span><br><span class="line">str := fmt.Sprintf(<span class="string">&quot;tmp-mr-%v-%v&quot;</span>, mapTaskId, i)</span><br><span class="line"></span><br><span class="line"><span class="comment">//3. Sscanf 从字符串格式化读入数据; n表示读到了几个数据</span></span><br><span class="line">n, err := fmt.Sscanf(str, <span class="string">&quot;mr-%v-%v&quot;</span>, &amp;map_id, &amp;reduce_id)</span><br><span class="line"></span><br><span class="line"><span class="comment">//4. Fprintf 向文件中格式化输出数据</span></span><br><span class="line">fmt.Fprintf(filePtr, <span class="string">&quot;%v %v\n&quot;</span>, key, val)</span><br></pre></td></tr></table></figure>
</li>
<li><p>json</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1. 直接把struct数据写入到文件中，以json格式</span></span><br><span class="line">encoder := json.NewEncoder(filePtr)</span><br><span class="line">err := encoder.Encode(kvaStruct)</span><br><span class="line"></span><br><span class="line"><span class="comment">//2. 从文件中把json解码，并读入为struct</span></span><br><span class="line">decoder := json.NewDecoder(f)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">		kv := KeyValue&#123;&#125;</span><br><span class="line">		<span class="keyword">if</span> err := decoder.Decode(&amp;kv); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		&#125;</span><br><span class="line">		kva = <span class="built_in">append</span>(kva, kv)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>读写文件相关</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.利用ioutil读文件，得到的buf类型为[]byte；readAll读入该文件所有的内容</span></span><br><span class="line">buf, err := ioutil.ReadAll(file)</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.利用ioutil读取目录，得到的是每个文件的描述struct(不是文件指针，也不是fileName)</span></span><br><span class="line">allFilesDescription, err := ioutil.ReadDir(<span class="string">&quot;./&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> _, fileDes := <span class="keyword">range</span> allFilesDescription &#123;</span><br><span class="line">  fileName := fileDes.Name()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3. 利用filePtr.Read只能读取缓冲区大小的数据</span></span><br><span class="line">buf := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">1024</span>)</span><br><span class="line">n, err := filePtr.Read(buf)  <span class="comment">//读到的只是1024 bytes大小的数据，不会读完全文</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
  </entry>
  <entry>
    <title>lab2-raft</title>
    <url>/2021/04/13/lab2-raft/</url>
    <content><![CDATA[<p>最近完成了6.824的lab2，记录一下笔记。</p>
<p>lab2的内容是实现raft协议。整个实验的内容分为3部分，lab2-A实现raft的选举机制，lab2-B实现raft的日志追加机制，lab2-C实现raft的持久化。</p>
<span id="more"></span>

<p>完成实验的过程整体上可以分为两方面，其一是理解raft算法原理和细节，其二是设计和具体实现。</p>
<p>以下内容也从这两方面分别介绍</p>
<h2 id="raft-算法原理和细节"><a href="#raft-算法原理和细节" class="headerlink" title="raft 算法原理和细节"></a>raft 算法原理和细节</h2><p>为了对大规模数据进行处理，单机就成为了瓶颈，解决的办法就是使用更多的机器协同工作。而由于计算机本身的物理特性，其存在一定的崩溃概率，另外计算机网络也是计算机集群工作时出现故障的潜在原因（网络分隔、数据包丢失、数据包未按序到达）。如何使得由网络连接的计算机之间能协调一致工作，在某个机器故障or网络故障后，整个系统依然能正常运转，这就是一致性算法的目标。一致性算法用来组织机器执行指令，并使得其最终状态是一致的，在这个过程中能容许部分节点的故障or失败。</p>
<p>raft是一种管理复制日志的一致性算法，它主要分为三个部分，选举、日志复制、安全性。raft算法会从整个集群中选举出一个leader，其它节点称为follower。每次客户端的command会先记录在leader的日志中，然后由leader负责同步给follower。当一条日志同步数目过半后，leader就可以commit这条日志，并把它应用到自己的状态机。说白了采用raft算法的系统中是采用投票机制来达成共识的，因此3个节点的系统中，只要2个节点一致了，整个系统就算做达成了共识。所以3个节点的系统可以容忍1个节点故障，5个节点的系统可以容忍2个节点的故障。</p>
<p>下图是raft原论文中的，理解了这张图，外加一些选举安全性的说明，实现raft算法就基本没问题了。</p>
<img src="../images/raft0.png" alt="raft0" style="zoom:80%;" />

<h3 id="选举"><a href="#选举" class="headerlink" title="选举"></a>选举</h3><p>raft使用心跳机制来实现leader的感知。即leader周期性的广播心跳信息给follower，follower由此得知leader的存在。如果follower在一段时间内没有收到心跳数据(可能是leader和它之间的网络中断了，也可能是leader故障了)，则它会认为leader已经死了，因此自己会变成candidate，并发起投票。如果它可以得到半数的票，就可以当选为新的leader。</p>
<p>follower变成canditate之后立即发起投票，这包含如下几个动作：term加一，自己投自己，重置自己的选举计时器，然后群发自己的请求投票RPC。</p>
<p>接收者收到candidate的投票请求后，先检查term。term相当于一个逻辑时钟，标志着现在发起投票的人处于哪一时代。如果term比自己的老，则可以直接拒绝。否则，结合日志的term、index(candidate的日志必须和自己的一样新or比自己的还新)，voteFor(未投过票就投给它，投过它就直接返回给它true，已经投给别人了就返回false)来判断是否投票给它。</p>
<p>当candidate变成leader以后，他需要立即群发心跳信息，以阻止其它人继续选举。相应地，如果一个节点收到了term比自己新的心跳信息，它就知道已经有leader了，自己就需要老老实实做follower。</p>
<p>为避免大家同时开始发起选举，然后都自己给自己投票，陷入僵局，可以把每个人的选举计时器都设置为随机时间，比如在300ms到400ms之间浮动。</p>
<p><strong>对于任何接收者来说，如果接收到的RPC消息中的term比自己的term大，自己就需要转变为follower</strong></p>
<p>另外，voteFor什么时候重置为空呢？由voteFor的定义可以知道，它是<strong>代表本节点在当前term投票给了谁</strong>，因此代码中任何发生Term变动的地方，都需要重置voteFor。</p>
<h3 id="复制日志"><a href="#复制日志" class="headerlink" title="复制日志"></a>复制日志</h3><p>上层应用把cmd发送给leader，然后leader把它写入自己的日志，然后把自己的日志传送给集群中的大多数节点。由于<strong>由于心跳和日志追加都是leader发送给其他人的，因此两者可以合并起来，当追加日志的RPC中log为空，就表示这是心跳信息，否则这既是心跳信息，也是日志追加信息。</strong>具体实施就是：每次该发送心跳的时候，如果有需要传送的日志，就带上，没有就只发送空消息。</p>
<p>复制日志的大体流程是：上层应用调用leader的start，把cmd传送给它。leader把cmd添加到自己的log中，在下一次心跳计时器到期后，发送给follower，follower会回复是否成功复制到自己的log，如果成功，则leader在自己的matchIndex数组记录一下。根据matchIndex，如果有半数以上的节点都复制了这条消息，leader就把它commit掉。在下一次心跳时，leader会告诉每个follower自己的commit的进度，然后follower会跟随leader也commit。</p>
<p>会有一个后台线程，不断把commit的cmd应用到状态机，一旦一条日志应用到了状态机，它就可以被返回给上层应用。</p>
<p>由于集群中任何时刻都可能有节点故障，因此存在follower的日志和leader的日志不一致的情况。根据raft的思想，以leader为主，冲突的follower的日志需要服从leader的日志（这是因为leader是得到大多数选票而选出来的，这也意味着它的日志比大多数人的都要新or一样新），服从leader也就是少数服从多数。</p>
<p>处理冲突需要leader和follower协商，一直找到双方都一致的那个点，然后重传从那个点以后的所有日志。协商机制可以通过leader的nextIndex数组实现。nextIndex数组在leader刚选出来的时候初始化为leader的日志末尾的下一个index，然后leader开始发送log后，如果收到follower的回复为false，表示这些日志没有匹配上，因此leader开始递减nextIndex，发送新的从nextIndex到末尾的日志过去，这样反复直到匹配成功。匹配的原理是，leader的某个index上的日志，必须和其它节点在该index上的日志一致。而我们不能靠检查日志内容来断定是否一致，而是通过Term来判断。即不同节点上的确定的index上的日志的term相等，则他们一致。</p>
<p>matchIndex表示双方截至目前匹配成功的index，在leader刚选举出来的时候初始化为0，表示leader不知道当前的匹配情况，然后可以随着leader和follower协商处理冲突日志的成功，而得知当前的匹配进度，从而更新matchIndex。matchIndex表示了leader和集群内所有节点的一致性匹配情况，因此可以用于提交log。当leader检查matchIndex，发现某一条日志被复制到了大多数节点后，就可以更新commitIndex，实现提交。<strong>当然，根据论文中的fig8，提交还有一个小细节，即leader更新commitIndex的这一条日志的Term必须是当前Term，不能是旧的Term，这一点随后解释</strong></p>
<h3 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h3><ol>
<li><p>选举安全：领导人的log中必须包括所有已提交的条目，换句话说，只有包含所有已提交的条目的节点才能被选为领导人。</p>
<p>实现这一点，是通过投票限制实现的：follower节点只会给日志比自己新or和自己一样新的人投票。这里的新的定义是：看最后一条日志，如果其Term更大，那就更新。如果Term相等，就看日志Index的大小或者说日志的长度，长度更长的就更新。</p>
<p>因此为了方便比较，candidate会在请求投票的RPC中携带最后一条日志的term和index，方便follower比较。</p>
</li>
<li><p>Leader不允许提交之前Term的日志条目，只能提交当前term的日志条目。</p>
<img src="../images/raft1.png" alt="raft1" style="zoom:70%;" />

<p>如上图所示，s1在term1时当选为leader，复制了2条消息，然后故障崩溃。s5在term2时当选为leader(凭借s3 s4和自己的票)，然后复制了1条消息，后故障崩溃。然后s1在term3时当选为leader，把黄色的消息给s3复制了一份，此时黄色消息已经在集群中复制过半了。<strong>如果允许s1作为leader提交这条黄色的消息</strong>，则存在可能，s1将消息应用到了自己的状态机，而s2和s3还没来得及commit or 应用黄色消息，就被图(d)中新当选的leader s5覆盖成了蓝色消息，随后提交蓝色消息并apply。此时，系统中出现了两种状态，在index=2的位置，s1为黄色消息，s2位蓝色消息。</p>
<p>为了应对这种不一致现象，<strong>规定，图(c)时刻，s1作为leader只能通过提交自己term的红色消息的方式间接提交旧term的消息，即s1观察到红色消息复制到了大多数，然后推进commitIndex为红色消息的index，从而间接把之前term的黄色消息也提交了。而不能直接通过观察到黄色消息已经过半，而推进commitIndex为黄色消息的index，因为这有可能会导致不一致状态</strong></p>
</li>
</ol>
<h2 id="lab2实现"><a href="#lab2实现" class="headerlink" title="lab2实现"></a>lab2实现</h2><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>系统中主要有三个线程负责系统的功能，分别是：heartBeat线程，负责定期发送心跳信息or追加日志；election线程，当选举计时器超时后，负责发起选举；apply线程，负责在后台定期监控当前是否有可以apply的日志(当applyIdx &lt; commitIdx时表示有可以应用的日志)。这三个线程以loop死循环的形式存在，直到上层应用发起kill命令后才退出。</p>
<p>loop内部需要周期性执行handle函数，采用了go内置的定时器，如下</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">electionMonitorLoop</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.stopCh:</span><br><span class="line">			<span class="keyword">return</span></span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.electionTimer.C:</span><br><span class="line">			rf.electionHandle()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">heartBeatSendLoop</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.stopCh:</span><br><span class="line">			<span class="keyword">return</span></span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.heartBeatTimer.C:</span><br><span class="line">			rf.heartBeatHandle()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">applyLoop</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.stopCh:</span><br><span class="line">			<span class="keyword">return</span></span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.applyTimer.C:</span><br><span class="line">			rf.applyHandle()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当计时器到期后，进入对应的handle函数，并在函数内部先重置自己的计时器，然后处理相应的事件。</p>
<p>关于timer的重置，有一些坑，下面的版本是基本正确的版本。坑主要在：当正在执行某个handle时，有可能新的定时的时间又到期了，这时底层会把信号写入到timer.C的channel中，此时直接重置timer是没用的，因为信号已经写入channel了，下个循环依旧会执行handle函数，想要下个循环不执行这个handle函数，需要把这个信号从channel消耗掉。</p>
<p>但是直接消耗也不对，因为stop函数返回false有两种可能，其一是timer到期了，信号写入channel，但没有被消耗；其二是timer到期了，信号写入channel，已经被消耗了。如果看到stop为false，就直接去消耗channel，就可能会阻塞在这里。因此可以采用select的方式，如果当前channel有数据，就消耗，如果没有，就走default分支。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rf *Raft)</span> <span class="title">resetHeartBeatTimer</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> !rf.heartBeatTimer.Stop() &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="keyword">case</span> &lt;-rf.heartBeatTimer.C:</span><br><span class="line">		<span class="keyword">default</span>:</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	rf.heartBeatTimer.Reset(HeartBeatTimeoutMs)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="一些bug"><a href="#一些bug" class="headerlink" title="一些bug"></a>一些bug</h3><ol>
<li><p>选举：candidate发送请求投票的RPC并返回结果、统计票数时，不必等所有的RPC都返回结果了再统计数据，尤其是不能阻塞等所有RPC都返回才统计，因为集群中可能存在网络故障or节点故障，某个RPC可能很久才返回结果or永远都无法返回结果。正确的做法是，统计支持的票数和返回的票数，支持票数过半即判定为成功，返回票数达到一半即判定为失败。</p>
</li>
<li><p>选举：RPC调用相当耗时，注意在RPC调用前释放锁。</p>
</li>
<li><p>日志追加：go语言的slice是引用，a := b[1:3]，a仅仅是指向b的引用，不是拷贝。因此在准备appendEntry的RPC的参数时，参数里的日志必须显式调用copy，从raft节点上复制日志，不能直接用引用的方式。否则，由于rpc的call之前会释放锁，而rpc参数又引用了raft的log数据，会发生data race</p>
</li>
<li><p>日志追加：follower节点根据leader的log数据覆盖自己的log的时候，需要考虑到多个RPC数据的先后顺序不一致的问题。比如，leader先发送[1,2]，后发送[1,2,3,4]。但由于网络拥塞，长日志先到，短日志后到，如果在follower端没有处理好如何覆盖的问题，有可能会导致短日志覆盖了长日志。</p>
<p>也不能仅仅覆盖leader发来的日志数据。假如follower中数据为[1,2,3,4,a,b,c,d]，从1开始冲突，而leader发送的数据为[101,102,103,104]，如果仅仅覆盖的话，follower会变成[101,102,103,104, a,b,c,d]，而我们期待的是[101,102,103,104]。</p>
<p>因此正确的做法是：拿到leader的log数据后，先按顺序匹配，直到匹配到不一致的地方，表示从此处到结尾的数据都是冲突的，删除从此处一直到结尾的数据，再把leader log匹配剩下的数据追加到follower</p>
</li>
<li><p>日志追加：需要考虑到网络延迟、数据包未按序到达的情况。日志追加时，leader收到follower的reply的顺序不一定是按序的。比如先发送[1:5]的log数据，许久未收到对应的reply，后leader收到上层通过start发送的消息多了几条，在下次心跳时发送了[1:10]的数据给follower。过了很久，[1:10]的reply先到，leader将这个follower的matchIndex改为10，随后[1:5]的reply后到，leader此时不能把matchIndex再改为5。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>lab3-Fault tolerant Key/Value Service</title>
    <url>/2021/04/19/lab3-Fault-tolerant-Key-Value-Service/</url>
    <content><![CDATA[<p>MIT6.824分布式原理的课程lab，这篇文章记录lab3的详情。</p>
<p>lab3的实验分为两个任务，一是基于lab2完成的raft协议，完成一个不带日志压缩的 Key Value Service系统；二是在已经搭建好的Key Value Service上实现日志压缩(也即 snapshot)。</p>
<span id="more"></span>

<h2 id="Key-value-service-without-log-compaction"><a href="#Key-value-service-without-log-compaction" class="headerlink" title="Key/value service without log compaction"></a>Key/value service without log compaction</h2><p>Key Value Service 由客户端和服务端组成，其中客户端处于用户的机器上，服务端和它底层的raft处于服务器上，并且每个服务端与对应的raft处于同一个机器，即raft作为底层，负责一致性，而服务端处于上层，负责接受从客户端发来的请求并结合raft的反馈对状态机做相应的修改。</p>
<p>通常，集群由n个服务端组成，只有一个服务端对应的底层raft为leader，也即只有这个服务端是leader。客户端的交互对象就是这个leader服务端。客户端发过来一个command给leader服务端，然后leader服务端把这条command拿去给集群内的follower达成共识，然后commit，一旦commit后，leader就可以把这个command执行（应用）到自己的状态机。</p>
<p>在客户端看来，就好像服务端并没有分布式集群，它仅仅是在和leader进行交互。</p>
<p>具体到本次lab，服务端负责维护的是一个kv数据库，在lab中我们将其简化为一个map[string] string。服务端需要处理三种请求：</p>
<ul>
<li>GET(key)：负责接受一个key，并查询其对应的value，如果没有找到，就返回空字符串。</li>
<li>PUT(key,value)：接受kv pair，并将其放入kv数据库。(如果以前有，就覆盖，如果以前没有，就新增)</li>
<li>APPEND(key,value)：接受kv pair，并将其append到数据库。(以前有，就将新value追加在旧value后面，以前没有，就直接新增)</li>
</ul>
<p>需要注意的是，<strong>Raft需要实现线性化语义</strong>。即作为客户端，只有在成功收到本次command的结果后，才会发起下一次command。不会出现一次性并行发送n条command，然后分别等待它们的结果的情况。</p>
<h3 id="客户端设计"><a href="#客户端设计" class="headerlink" title="客户端设计"></a>客户端设计</h3><p>客户端应该向集群中的leader发送command，但刚开始发送Get、Put之类的时候，它并不知道谁是leader。因此可以采用轮询的策略，当对方不是leader是，客户端会收到 ErrWrongLeader 的错误，这时它就可以向下一个节点发送这条命令了。</p>
<p>raft的线性化语义也必须在客户端得到实现，即：客户端在本条指令未得到结果前不会立刻return。比如下面的Get方法，如果网络拥塞导致超时，那就等待一个时间段然后重试；如果是错误的leader，就重新定位到下一个集群节点，并发送请求；如果是服务端超时，则可能是定位到了partition的leader上，导致command迟迟无法commit，也需要换一个节点发送请求；直到收到正确的结果后才return</p>
<p>另外，对于同一条command，客户端可能会在网络中发送不止一条，所以需要给command编号。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ck *Clerk)</span> <span class="title">Get</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">  </span><br><span class="line">	request := GetArgs&#123;Key: key, ClientId: ck.clientId, OperationId: ck.operationId&#125;</span><br><span class="line">	ck.operationId++</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		response := GetReply&#123;&#125;</span><br><span class="line">		ok := ck.servers[ck.leaderId].Call(<span class="string">&quot;KVServer.Get&quot;</span>, &amp;request, &amp;response)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> ok == <span class="literal">false</span> &#123;</span><br><span class="line">			ck.leaderId = (ck.leaderId + <span class="number">1</span>) % <span class="built_in">len</span>(ck.servers)</span><br><span class="line">			time.Sleep(RetrySendReqTimeMs)</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> response.Err == ErrNoKey &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> response.Err == ErrWrongLeader &#123;</span><br><span class="line">			ck.leaderId = (ck.leaderId + <span class="number">1</span>) % <span class="built_in">len</span>(ck.servers)</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> response.Err == ErrRaftTimeout &#123;</span><br><span class="line">			<span class="comment">//超时，maybe定位到了partition的旧leader上</span></span><br><span class="line">			ck.leaderId = (ck.leaderId + <span class="number">1</span>) % <span class="built_in">len</span>(ck.servers)</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> response.Err == OK &#123;</span><br><span class="line">			<span class="keyword">return</span> response.Value</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			log.Fatal(<span class="string">&quot;wrong GET reply args&quot;</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="服务端设计"><a href="#服务端设计" class="headerlink" title="服务端设计"></a>服务端设计</h3><p>服务端接收从客户端发来的command，并把command通过Start交给raft层。如果raft发现自己不是leader，就会拒绝这个command，并return false，服务端得到false后需要将 ErrWrongLeader 错误反馈给客户端。</p>
<p>如果start成功，则说明command已经交给raft层，由raft层负责达成共识。在达成共识后会通知服务端，服务端就可以apply或者说执行这条command了。</p>
<p>由于考虑到效率问题，整个过程应该是并发的，而考虑到raft层达成一致需要一段时间，因此start 一条command、执行command、反馈结果给客户端 这三个过程应该是异步的。</p>
<img src="../images/kv-Service.jpg" alt="kv-Service" style="zoom:60%;" />

<p>如上图所示，当一个command到达服务端的RPC接口后，服务端将command交给raft层的start函数，并启动一个定时器，然后等待结果。raft层在达成一致后，会把command写入raft.applyCh。服务端的apply Loop收到通知后，开始执行这条command并把结果应用到自己的数据库中。并把结果通过service.applyCh通知到RPC接口，RPC接口收到通知后，返回结果给客户端。如果RPC接口函数迟迟得不到通知，超时定时器就会到期，此时接口函数直接给客户端返回ErrRaftTimeout错误。</p>
<p>图中的raft.applyCh只有一个，用于raft层向service层通知。而service.applyCh有多个，即每次RPC请求，就新创建一个，用于服务端的service Loop向服务端的RPC接口函数通知command结果。 </p>
<p>可以看到，raft层只是用来达成一致的，当它认为集群中过半数的节点都同步了某条指令，它就会使用applyCh把它通知给Service层，告诉service层，你可以放心的执行这条command了。</p>
<p><strong>序列号：</strong>客户端有可能会重发某条指令（比如网络拥塞导致客户端主动重发）这就要保证重发的指令不能被又一次执行，必须返回上次执行过的结果。解决办法就是给command编号，具体来说，使用&lt;clientId, commandId&gt; pair来标识每一条command，其中clientId表示客户端的识别号，commandId表示指令的Id，指令Id是递增的。服务端记录自己最近已经apply到了哪一条指令，当有新指令需要处理时，先检查它的序列号Id是否小于自己上一次处理过的，如果小于，则说明这条指令已经处理过了，直接给用户回复Ok即可(丢弃也行，因为既然更大的command都已经处理过了，说明之前的command用户早已收到结果了)。</p>
<h2 id="Key-value-service-with-snapshots"><a href="#Key-value-service-with-snapshots" class="headerlink" title="Key/value service with snapshots"></a>Key/value service with snapshots</h2><p>partB要求在key value Service 的基础上完成快照功能。</p>
<img src="../images/snapshot.png" alt="snapshot" style="zoom:60%;" />

<img src="../images/snapshotRPC.png" alt="snapshotRPC" style="zoom:60%;" />

<p>在正常运行的过程中，raft会将自己的log持久化，当系统崩溃并重启后，由于没有持久化service层的状态机，所以整个系统都是0状态，因此raft会将自己的commitIdx和applyIdx(这两个数据不需要持久化)重置为0，并从第一条log开始重新执行到最新的log位置。这会浪费大量的时间，并且为了恢复状态，需要保存从系统开机到当前的所有log，这是很占存储空间的。</p>
<p>快照snapshot解决了这个问题，即在某个时间节点把service层的状态机的数据全部保存一份(在这里就是service层的kv数据库)，当系统崩溃重启后，只需要先把snapshot恢复到service层，然后再依次执行快照时间点之后的所有log。</p>
<img src="../images/snapshotServiceRaft.png" alt="snapshotServiceRaft" style="zoom:60%;" />

<p>整个快照机制如上图所示(图来自6.824实验的主页)。</p>
<h3 id="建立快照"><a href="#建立快照" class="headerlink" title="建立快照"></a>建立快照</h3><p>每个节点都会建立自己的快照，包括leader和follower。</p>
<p>service层会定期检测raft层保存在硬盘中的持久化数据的大小(主要是log太占空间)，一旦发现持久化数据文件超过了某个大小，就启动快照机制。service层首先将自己维护的数据库序列化，然后通知raft层将raft层的日志截断，丢弃截断的前面的log。从哪里截断呢？应该从service层保存自己快照的最后一条log截断。之后，将service层序列化的数据和raft层的需要持久化的一些状态一起写入硬盘。建立快照就ok了。</p>
<p>service层需要持久化的数据（写入快照的数据）：自己的数据库、lastApplyIndex、lastApplyTerm、recentApplyOpId</p>
<p>raft层需要持久化的数据和lab2一样，仍然是那些</p>
<h3 id="发送快照"><a href="#发送快照" class="headerlink" title="发送快照"></a>发送快照</h3><p>快照的发送由leader到follower。通常发生在leader发送心跳数据(包括append log)时，leader发现follower的log落后于leader太远时，会把自己的快照发送给follower。</p>
<p>具体到代码实现上，leader平时使用nextIndex数组来和follower协商，下次发送从哪里开始的log。当leader发现，nextIndex数组已经匹配到自己已经抛弃的log的index时(这些log在建立好快照后被抛弃)，就需要发送snapshot数据而非发送log数据。具体的RPC参数可见原论文的图。</p>
<h3 id="安装快照"><a href="#安装快照" class="headerlink" title="安装快照"></a>安装快照</h3><p>安装快照涉及到service层和raft层的互动，比较麻烦。</p>
<p>首先由Leader向follower发送快照数据。follower收到这些数据后，向service层的applyCh写入一条通知，通知的内容就是需要安装快照。然后service层首先调用raft层的condInstallSnapshot，让raft判断是否安装这个快照（如果raft层已经拥有这个快照的状态对应的最后一条log，或者raft层已经有比它更新的快照），就不用安装这个快照。否则，需要安装快照。raft层需要将applyIndex设置为快照状态对应的最后一条log的index，然后返回true。service层收到true后，将快照读取，并利用读取的数据重置自己的状态。</p>
<p>全过程中，raft需要重置的数据，是applyIndex。而commitIndex不用重置，因为commitIndex是从leader处获得的。</p>
<h2 id="一些bug"><a href="#一些bug" class="headerlink" title="一些bug"></a>一些bug</h2><ol>
<li>channel不能加锁。这个和RPC调用不能加锁一样，channel加锁可能导致死锁。不知道为什么前几次实验都通过了，这个bug一直到lab3才暴露出来。</li>
</ol>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><img src="../images/lab3A.png" alt="lab3A" style="zoom:40%;" />

<img src="../images/lab3B.png" alt="lab3B" style="zoom:40%;" />]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
</search>
